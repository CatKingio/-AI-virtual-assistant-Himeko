{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed with re pattern and stop words:\n",
      "                          input       output\n",
      "0     [Hello, ,, how, are, you]  [greetings]\n",
      "1  [I, am, fine, ,, thank, you]  [responses]\n",
      "\n",
      "Processed without re pattern but with stop words:\n",
      "                             input       output\n",
      "0     [Hello, ,, how, are, you, ?]  [greetings]\n",
      "1  [I, am, fine, ,, thank, you, !]  [responses]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.use_re = False\n",
    "        self.stop_words = set()\n",
    "    \n",
    "    def preprocess_text(self, text, re_pattern=None, stop_words=None):\n",
    "        words = word_tokenize(text)\n",
    "        if self.use_re and re_pattern:\n",
    "            pattern = \"[\" + re.escape(re_pattern) + \"]\"\n",
    "            text = re.sub(pattern, '', text)\n",
    "            words = word_tokenize(text)\n",
    "        if self.stop_words and stop_words:\n",
    "            words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return words\n",
    "\n",
    "    def tokenize_df(self, df, input_column, output_column, re_pattern=None, stop_words=None):\n",
    "        if re_pattern:\n",
    "            self.use_re = True\n",
    "        else:\n",
    "            self.use_re = False\n",
    "        if stop_words:\n",
    "            with open(stop_words, \"r\", encoding=\"utf-8\") as file:\n",
    "                self.stop_words.update(line.strip() for line in file)\n",
    "        else:\n",
    "            self.stop_words = set()\n",
    "\n",
    "        df['input'] = df[input_column].apply(lambda x: self.preprocess_text(x, re_pattern, stop_words))\n",
    "        df['output'] = df[output_column].apply(lambda x: self.preprocess_text(x, re_pattern, stop_words))\n",
    "\n",
    "        df.drop(columns=[input_column, output_column], inplace=True)\n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    'patterns0': ['Hello, how are you?', 'I am fine, thank you!'],\n",
    "    'tag': ['greetings', 'responses']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# With re pattern and stop words\n",
    "df_processed_with_re = tokenizer.tokenize_df(df.copy(), 'patterns0', 'tag', \"[.;\\]=]!?\", \"vietnamese.txt\")\n",
    "print(\"Processed with re pattern and stop words:\")\n",
    "print(df_processed_with_re[['input', 'output']])\n",
    "\n",
    "# Without re pattern but with stop words\n",
    "df_processed_with_stop_words = tokenizer.tokenize_df(df.copy(), 'patterns0', 'tag', stop_words=\"vietnamese.txt\")\n",
    "print(\"\\nProcessed without re pattern but with stop words:\")\n",
    "print(df_processed_with_stop_words[['input', 'output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[9, 10, 11, 3, 12, 6, 13]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       input output\n",
       "0         [2, 3, 4, 5, 6, 7]      8\n",
       "1  [9, 10, 11, 3, 12, 6, 13]     14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = {}\n",
    "index = 2\n",
    "\n",
    "# Gán chỉ số riêng biệt cho mỗi từ trong cột \"patterns\" và nhãn trong cột \"tag\"\n",
    "for i, row in df_processed_with_stop_words.iterrows():\n",
    "    indices = []\n",
    "    for word in row['input']:\n",
    "        if word not in index_dict:\n",
    "            index_dict[word] = index\n",
    "            index += 1\n",
    "        indices.append(index_dict[word])\n",
    "    df_processed_with_stop_words.at[i, 'input'] = indices\n",
    "    \n",
    "    label = row['output'][0]\n",
    "    if label not in index_dict:\n",
    "        index_dict[label] = index\n",
    "        index += 1\n",
    "    df_processed_with_stop_words.at[i, 'output'] = index_dict[label]\n",
    "\n",
    "\n",
    "df_processed_with_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dạ con', 'sáng rõ', 'ừ ào', 'chao ôi', 'gây giống', 'bấy giờ', 'có ngày', 'tới gần', 'bao lâu', 'lần khác', 'bất kỳ', 'ngay từ', 'nhân tiện', 'nhìn xuống', 'xon xón', 'thốc', 'nhân dịp', 'chứ không phải', 'tìm việc', 'ra vào', 'chết thật', 'không nhận', 'alô', 'dở chừng', 'ở đây', 'tên chính', 'ví phỏng', 'nhiều', 'từ thế', 'bỗng không', 'hiện tại', 'tính căn', 'tìm', 'lên ngôi', 'biết mình', 'thậm từ', 'gì', 'đúng ra', 'mở mang', 'rồi nữa', 'phóc', 'cơ cùng', 'chu cha', 'xem số', 'ngồi không', 'bỏ mẹ', 'loại từ', 'vụt', 'không ai', 'đạt', 'sau đây', 'quả', 'chậc', 'ắt là', 'nhờ chuyển', 'tránh', 'trước kia', 'cả đến', 'chùn chũn', 'lâu ngày', 'cật lực', 'giờ lâu', 'chúng mình', 'chứ như', 'chuyển tự', 'được nước', 'lấy thêm', 'lên', 'quả là', 'tính từ', 'đáng lí', 'nào phải', 'chăng', 'là thế nào', 'sao cho', 'chung nhau', 'sì', 'cảm thấy', 'phía sau', 'ba họ', 'làm bằng', 'tuy có', 'hơn', 'không được', 'tuy rằng', 'hay', 'dễ nghe', 'nghe đâu', 'qua chuyện', 'mọi nơi', 'anh ấy', 'cụ thể là', 'lấy ra', 'khó khăn', 'trong vùng', 'cây nước', 'lần trước', 'trực tiếp', 'ắt', 'tiếp tục', 'sắp', 'sáng thế', 'làm cho', 'thường xuất hiện', 'cô mình', 'cụ thể như', 'ba tăng', 'ngoải', 'rốt cuộc', 'nếu không', 'một ít', 'tuổi', 'dùng cho', 'phải rồi', 'rốt cục', 'tính', 'tức thì', 'bắt đầu', 'rồi tay', 'chỉ có', 'bán thế', 'căn cái', 'thích ý', 'không tính', 'đã là', 'gặp phải', 'con con', 'chị', 'chuyển đạt', 'người nghe', 'thôi việc', 'chốc chốc', 'chứ còn', 'rút cục', 'nhằm vào', 'nếu thế', 'con dạ', 'khoảng cách', 'những như', 'tắp', 'xa xa', 'không hay', 'từ loại', 'ổng', 'thế là', 'lấy được', 'chăn chắn', 'ấy là', 'gần như', 'vậy mà', 'đều nhau', 'là cùng', 'thỉnh thoảng', 'tối ư', 'tấn tới', 'giữ ý', 'nghĩ', 'chính giữa', 'nặng căn', 'điểm chính', 'cứ', 'là vì', 'lấy', 'giảm thế', 'nớ', 'chung cuộc', 'xiết bao', 'giữa', 'nói chung', 'không', 'từ tại', 'còn thời gian', 'không để', 'đưa tới', 'xuất hiện', 'thực tế', 'ra lại', 'không cần', 'ai nấy', 'làm riêng', 'chắc', 'lấy ráo', 'chuyện', 'ngày ngày', 'cách', 'bông', 'lòng không', 'thương ôi', 'cóc khô', 'nhớ lấy', 'vạn nhất', 'phải', 'bỏ việc', 'họ', 'sớm ngày', 'đều đều', 'còn như', 'ối giời ơi', 'dần dần', 'qua ngày', 'tuy là', 'mở nước', 'chẳng lẽ', 'ối dào', 'mới hay', 'lên đến', 'cho nhau', 'nghiễm nhiên', 'nói rõ', 'nhất tâm', 'nơi nơi', 'có đáng', 'vừa rồi', 'em em', 'sự việc', 'chợt nhìn', 'nói lại', 'xem lại', 'cha', 'cao thấp', 'bên cạnh', 'càng càng', 'dẫu sao', 'bỏ cha', 'thì thôi', 'nào đó', 'như trước', 'văng tê', 'suýt', 'thoạt nhiên', 'bắt đầu từ', 'riêng từng', 'cao lâu', 'tại', 'bất tử', 'cho được', 'nhận được', 'tìm hiểu', 'tháng năm', 'nhất nhất', 'không phải không', 'nước quả', 'trời đất ơi', 'khác khác', 'hay tin', 'sẽ', 'mang', 'rất lâu', 'hết của', 'dài ra', 'không còn', 'cũng vậy', 'phỏng nước', 'ý hoặc', 'lấy có', 'tháng', 'vùng lên', 'sau đó', 'sau cuối', 'thế chuẩn bị', 'chưa', 'cùng chung', 'riêng', 'cho đến nỗi', 'gần', 'gặp khó khăn', 'vậy là', 'chung qui', 'ra tay', 'xoành xoạch', 'đến nỗi', 'chắc ăn', 'giá trị', 'nói là', 'sang năm', 'người hỏi', 'thoắt', 'như vậy', 'lần này', 'thật', 'giảm chính', 'đưa xuống', 'chỉn', 'bởi vì', 'toé khói', 'trong khi', 'sắp đặt', 'khi', 'từ căn', 'đúng với', 'thứ', 'không kể', 'thường số', 'và', 'nghe hiểu', 'luôn', 'càng hay', 'sử dụng', 'trước đây', 'căn tính', 'đặt mức', 'trong lúc', 'bởi ai', 'ô hay', 'vô kể', 'làm vì', 'vả chăng', 'mỗi người', 'thì giờ', 'ngay cả', 'ừ', 'có người', 'cuối cùng', 'ở năm', 'còn về', 'thửa', 'đáng', 'tất cả', 'nghĩ ra', 'bập bà bập bõm', 'bỗng đâu', 'thật chắc', 'dễ sử dụng', 'chưa cần', 'cuộc', 'nặng', 'cả nhà', 'thốt', 'vài nơi', 'nói đủ', 'ít quá', 'chớ chi', 'nhìn', 'thay đổi tình trạng', 'bản', 'nghĩ lại', 'những muốn', 'đã hay', 'ít ra', 'bất đồ', 'tuốt tuột', 'phải cách', 'thôi', 'nhớ bập bõm', 'ráo trọi', 'tuốt luốt', 'dạ', 'trên', 'làm nên', 'mới đây', 'rõ thật', 'cậu', 'thuần ái', 'á à', 'hơn cả', 'luôn luôn', 'qua tay', 'thuộc bài', 'tên họ', 'ăn làm', 'chịu chưa', 'xa', 'cho đang', 'chứ ai', 'là nhiều', 'vậy ra', 'cô tăng', 'thời gian sử dụng', 'không bán', 'phương chi', 'thiếu điểm', 'giống', 'ôi chao', 'xem ra', 'ông từ', 'đại loại', 'thêm chuyện', 'thảo hèn', 'tìm cách', 'nước đến', 'đủ điểm', 'anh', 'cho rồi', 'nghỉm', 'qua lần', 'quá tay', 'chứ', 'từ tính', 'vung tàn tán', 'dễ khiến', 'ăn', 'qua', 'bà', 'dễ thấy', 'làm lại', 'nghe chừng', 'lại nói', 'hỏi lại', 'ngày', 'chúng', 'như không', 'con nhà', 'đánh giá', 'các', 'để mà', 'lại người', 'ông ổng', 'đây rồi', 'một lúc', 'vèo vèo', 'ý da', 'này', 'chầm chập', 'của', 'bộ thuộc', 'phỏng theo', 'bất kì', 'thuộc', 'tắp tắp', 'nước bài', 'bây chừ', 'bằng được', 'tênh', 'cơ', 'tha hồ chơi', 'qua đi', 'sau sau', 'nói bông', 'tuy vậy', 'tựu trung', 'nên chi', 'nghe thấy', 'ngày tháng', 'giảm thấp', 'dùng hết', 'biết mấy', 'lớn nhỏ', 'trước khi', 'giờ đây', 'cho tới', 'ngày giờ', 'nữa là', 'bị', 'lời', 'cùng cực', 'quận', 'hay đâu', 'thanh không', 'thốt nói', 'tò te', 'tới nơi', 'quá trình', 'mọi sự', 'vài nhà', 'để lại', 'buổi ngày', 'phần nhiều', 'cô quả', 'ai', 'bấy lâu nay', 'rất', 'đến giờ', 'chính thị', 'phải khi', 'lấy lý do', 'đâu phải', 'cần', 'phía dưới', 'theo', 'thích', 'thanh điều kiện', 'bán cấp', 'cao thế', 'không điều kiện', 'mỗi lần', 'không những', 'đại phàm', 'chăng nữa', 'mọi việc', 'ra ngôi', 'biết đâu chừng', 'bên có', 'đây', 'xệp', 'hiện nay', 'đang thì', 'đại nhân', 'tất cả bao nhiêu', 'lượng số', 'khó chơi', 'ren rén', 'con tính', 'cụ thể', 'đưa ra', 'ông ấy', 'mọi', 'từng đơn vị', 'trước', 'ắt hẳn', 'tính cách', 'tạo', 'phỏng', 'nước cùng', 'mang mang', 'sang', 'ngày nào', 'cái họ', 'từ', 'theo bước', 'trong đó', 'cách bức', 'ví thử', 'ít hơn', 'đưa chuyện', 'vị tất', 'làm mất', 'dù gì', 'khác xa', 'lúc nào', 'thế nên', 'liên quan', 'yêu cầu', 'quay', 'tăng cấp', 'phải như', 'nhận thấy', 'ít', 'bỗng dưng', 'nhất sinh', 'dành', 'ông tạo', 'sự thế', 'dù sao', 'chọn bên', 'thà rằng', 'than ôi', 'về tay', 'như thể', 'người nhận', 'bỗng nhưng', 'chăng chắc', 'dễ', 'cả ngày', 'khi trước', 'lâu', 'đủ', 'ăn chắc', 'nhón nhén', 'bên bị', 'đâu có', 'chớ gì', 'lại quả', 'để giống', 'ngồi bệt', 'ừ ừ', 'có số', 'bản thân', 'đó', 'xa tanh', 'rứa', 'hay là', 'vài ba', 'phăn phắt', 'thêm giờ', 'với lại', 'vậy', 'cần cấp', 'ngoài xa', 'ở lại', 'lấy xuống', 'ngay bây giờ', 'bằng cứ', 'cả tin', 'chung quy lại', 'có ăn', 'nói khó', 'thì', 'dễ dùng', 'thường thường', 'thộc', 'hay biết', 'ý chừng', 'thanh chuyển', 'thế đó', 'người', 'tuốt tuồn tuột', 'tấn', 'bỏ không', 'đâu', 'ăn về', 'nhận nhau', 'bỏ', 'thực ra', 'thích cứ', 'trực tiếp làm', 'cơ chừng', 'tất tần tật', 'nghe được', 'đến thì', 'bởi nhưng', 'chắc lòng', 'từng thời gian', 'xuống', 'suýt nữa', 'nên người', 'trước nhất', 'thật là', 'tự khi', 'tốt bộ', 'vài tên', 'nữa', 'nhà làm', 'tuổi tôi', 'lúc này', 'lại còn', 'lòng', 'trước tuổi', 'ngăn ngắt', 'từ ấy', 'ngày này', 'ra bộ', 'xoẹt', 'ngày rày', 'thốc tháo', 'tanh', 'biết đâu', 'thời gian tính', 'ráo nước', 'nhiệt liệt', 'tấm', 'a ha', 'chung chung', 'mợ', 'xoẳn', 'cao răng', 'quá giờ', 'ít nhất', 'nào', 'chùn chùn', 'cách đều', 'thế thôi', 'bấy nhiêu', 'nhất mực', 'thấp xuống', 'áng', 'những ai', 'thoạt', 'những khi', 'lại ăn', 'oái', 'xử lý', 'nặng mình', 'sẽ hay', 'có điều kiện', 'chết nỗi', 'đặt mình', 'phải lại', 'ít có', 'bất luận', 'phía trước', 'điểm đầu tiên', 'là là', 'vung thiên địa', 'thanh thanh', 'hơn trước', 'cách nhau', 'vượt quá', 'thanh tính', 'chớ không', 'khác thường', 'vài', 'thế thì', 'đành đạch', 'nhỉ', 'phía bạn', 'đến ngày', 'vị trí', 'đã đủ', 'thình lình', 'hiểu', 'làm như', 'ra người', 'từng', 'nhất quyết', 'đang tay', 'tốt mối', 'cùng với', 'tông tốc', 'ra lời', 'tốt ngày', 'đâu đâu', 'việc gì', 'đến bao giờ', 'cả', 'cá nhân', 'về phần', 'riệt', 'nơi', 'thúng thắng', 'biết chừng nào', 'kể như', 'nếu được', 'mọi thứ', 'cao sang', 'lại thôi', 'tại đó', 'bộ điều', 'đến xem', 'trong mình', 'đưa cho', 'vô luận', 'ra đây', 'sì sì', 'vậy thì', 'tin vào', 'ạ', 'thuộc lại', 'úi chà', 'có phải', 'tuyệt nhiên', 'nặng về', 'xem', 'nói toẹt', 'bị chú', 'không có', 'đủ dùng', 'buổi sớm', 'quá ư', 'khoảng không', 'xoét', 'ngay tức khắc', 'cách không', 'ở đó', 'quay đi', 'xềnh xệch', 'thế mà', 'do vì', 'có nhiều', 'mỗi một', 'nóc', 'mà lại', 'vẫn thế', 'người khác', 'thế thường', 'đâu nào', 'lên mạnh', 'hơn hết', 'vào gặp', 'năm', 'cũng', 'đang', 'nghe trực tiếp', 'giữ lấy', 'lời nói', 'thái quá', 'đến khi', 'điểm', 'đặt ra', 'khó thấy', 'là', 'dùng đến', 'giữ', 'chung quy', 'như là', 'nhất thì', 'nếu cần', 'thốt thôi', 'thì ra', 'không khỏi', 'chưa bao giờ', 'bập bõm', 'quả thế', 'tính phỏng', 'tốt hơn', 'nước nặng', 'bởi', 'dữ cách', 'lại cái', 'tìm bạn', 'vào lúc', 'chơi họ', 'tên', 'tốt', 'có ý', 'chú mình', 'toà', 'thường hay', 'xăm xắm', 'sốt sột', 'cứ việc', 'phỏng như', 'hay không', 'sao đang', 'cổ lai', 'thứ bản', 'thêm', 'nói nhỏ', 'phải chi', 'ăn hỏi', 'chú dẫn', 'tiếp đó', 'dễ đâu', 'khi nên', 'lượng từ', 'rón rén', 'vậy ư', 'lần tìm', 'cơ mà', 'từ nay', 'làm tắp lự', 'lấy làm', 'trong này', 'tột cùng', 'đến lời', 'vừa lúc', 'ơ kìa', 'xin', 'việc', 'làm tôi', 'quay lại', 'theo tin', 'vung tán tàn', 'thấp thỏm', 'nhằm để', 'ở trên', 'mới', 'đơn vị', 'khác nhau', 'gần đây', 'rồi thì', 'tránh xa', 'xuất kì bất ý', 'nhà khó', 'làm gì', 'khó mở', 'nhớ lại', 'dạ dạ', 'cho đến khi', 'nên làm', 'đó đây', 'dù rằng', 'duy', 'bị vì', 'bỏ lại', 'vài điều', 'quá mức', 'vậy nên', 'tại sao', 'đại để', 'thật ra', 'tanh tanh', 'cũng như', 'bài', 'mới rồi', 'mạnh', 'vì sao', 'chú mày', 'muốn', 'đâu cũng', 'cao số', 'chứ lị', 'tạo cơ hội', 'buổi mới', 'bất quá', 'được tin', 'trển', 'giữa lúc', 'rén', 'đủ nơi', 'chọn ra', 'nếu mà', 'ở như', 'dài', 'ừ thì', 'ăn sáng', 'biết thế', 'nói phải', 'chưa dùng', 'lâu nay', 'cả người', 'đến cùng cực', 'nọ', 'nhận họ', 'gồm', 'không ngoài', 'có chăng', 'quá tuổi', 'xin gặp', 'chung ái', 'xăm xăm', 'sau cùng', 'giống như', 'phụt', 'nhớ ra', 'nói thật', 'cơn', 'hỏi xem', 'sáng', 'chui cha', 'làm sao', 'thảo nào', 'nhé', 'nhất tề', 'bước', 'làm lòng', 'cơ chỉ', 'bỏ bà', 'ăn chung', 'sau', 'hết ý', 'tại tôi', 'rõ là', 'nữa rồi', 'chẳng nữa', 'có thế', 'gây thêm', 'căn cắt', 'vào', 'là ít', 'nhau', 'dạ khách', 'mà không', 'nhỏ', 'bỗng nhiên', 'ra sao', 'khi khác', 'ờ', 'lại giống', 'ngay tức thì', 'pho', 'thường', 'ngày đến', 'ví bằng', 'vì thế', 'hầu hết', 'vừa mới', 'cho ăn', 'dễ ăn', 'thấp', 'trong số', 'ăn chịu', 'tớ', 'hay nhỉ', 'thẩy', 'ứ hự', 'lớn lên', 'đáo để', 'số phần', 'câu hỏi', 'ôi thôi', 'nhìn theo', 'bấy chừ', 'người khách', 'đưa', 'ạ ơi', 'không biết', 'nói tốt', 'khoảng', 'tên tự', 'cấp', 'như nhau', 'cơ hội', 'như quả', 'nhất luật', 'cái đó', 'mỗi lúc', 'dễ thường', 'đưa tay', 'trước nay', 'đáng kể', 'ra điều', 'thuần', 'giờ đến', 'chơi', 'dẫu mà', 'tù tì', 'lúc trước', 'nhóm', 'cu cậu', 'lên cơn', 'cái', 'nhất', 'không bao lâu', 'còn nữa', 'bạn', 'nhìn lại', 'thanh ba', 'xa xả', 'mình', 'đến nơi', 'sang sáng', 'nói thêm', 'nói qua', 'đến thế', 'nghĩ tới', 'nhằm khi', 'nhận ra', 'mọi lúc', 'sở dĩ', 'có chứ', 'trệu trạo', 'đưa về', 'đã', 'một cách', 'ơi là', 'tăng giảm', 'ngày cấp', 'nhận việc', 'bức', 'chị bộ', 'bỏ nhỏ', 'lên nước', 'con', 'thục mạng', 'ai đó', 'cái gì', 'bất quá chỉ', 'nói ý', 'phía', 'cần gì', 'làm dần dần', 'vâng dạ', 'mỗi', 'từng phần', 'có', 'ít lâu', 'thời gian', 'tới', 'nghĩ xa', 'chịu ăn', 'ngày ấy', 'bước khỏi', 'ăn người', 'ngoài ra', 'trả của', 'trả lại', 'sáng ý', 'có ai', 'bằng người', 'vấn đề quan trọng', 'ngay lập tức', 'đặt trước', 'những', 'gì gì', 'vấn đề', 'cơ hồ', 'răng', 'ngoài', 'ra chơi', 'chớ', 'chắc dạ', 'nói', 'vâng ý', 'tuy', 'dẫu rằng', 'nghe không', 'để đến nỗi', 'về nước', 'tình trạng', 'đều', 'mọi khi', 'lần theo', 'biết bao nhiêu', 'trước hết', 'dẫn', 'bển', 'nên', 'điểm gặp', 'luôn tay', 'trước đó', 'phải không', 'so', 'ra ý', 'quay số', 'cho thấy', 'xa tắp', 'dù', 'bây giờ', 'càng', 'tột', 'gì đó', 'thậm', 'hỏi xin', 'có vẻ', 'ừ nhé', 'kể cả', 'lại nữa', 'vèo', 'ngày xửa', 'nhận làm', 'veo veo', 'do vậy', 'em', 'cho đến', 'có thể', 'chuyển', 'xảy ra', 'bao giờ', 'ba ba', 'khác gì', 'thi thoảng', 'do', 'hơn nữa', 'khẳng định', 'công nhiên', 'lấy thế', 'biết', 'không bao giờ', 'biết đâu đấy', 'bằng như', 'nước', 'bài cái', 'nghe lại', 'chiếc', 'vâng', 'ào vào', 'tự cao', 'bớ', 'nhờ nhờ', 'sao bản', 'chẳng những', 'điều', 'phắt', 'mở ra', 'cái đã', 'thấy', 'không có gì', 'nghe đâu như', 'mà thôi', 'nhất đán', 'ngay khi', 'tọt', 'lên số', 'đúng ngày', 'phải cái', 'nghĩ đến', 'nhà chung', 'sa sả', 'sớm', 'qua thì', 'nên chăng', 'gần xa', 'nghe như', 'tất thảy', 'đáng lẽ', 'một khi', 'cấp số', 'tuy thế', 'sao', 'bán', 'ngoài này', 'nhằm lúc', 'nước ăn', 'kể tới', 'ồ', 'ngày qua', 'chưa tính', 'lại đây', 'đầy tuổi', 'sáng ngày', 'còn có', 'làm được', 'lớn', 'chịu', 'chợt nghe', 'có khi', 'ít nữa', 'thế', 'dần dà', 'lấy lại', 'bên', 'chứ không', 'trếu tráo', 'cả nghĩ', 'dễ ngươi', 'thứ đến', 'xuất kỳ bất ý', 'thật thà', 'vâng vâng', 'hãy', 'mất còn', 'sự', 'dễ như chơi', 'bất ngờ', 'chắc người', 'vâng chịu', 'phót', 'bài bỏ', 'khó tránh', 'không thể', 'lấy vào', 'cùng', 'trong', 'mang về', 'ngôi nhà', 'à này', 'khá', 'thường sự', 'nhờ có', 'sang tay', 'không đầy', 'nhà ngươi', 'trệt', 'bà ấy', 'nước xuống', 'chia sẻ', 'phía trong', 'nhất thiết', 'đưa em', 'gặp', 'ở nhờ', 'làm ra', 'hay làm', 'điều kiện', 'ông', 'quá bộ', 'tránh tình trạng', 'chủn', 'mỗi ngày', 'nhiều ít', 'người người', 'bán dạ', 'theo như', 'số cho biết', 'đâu như', 'số', 'tháng tháng', 'đã không', 'coi mòi', 'dì', 'ba bản', 'mang lại', 'ngọn', 'chứ gì', 'bất cứ', 'số cụ thể', 'nhà', 'chúng ta', 'nhanh lên', 'lúc đó', 'quan trọng vấn đề', 'cực lực', 'trước sau', 'chuẩn bị', 'tiện thể', 'để', 'phần việc', 'như ý', 'chính', 'chắc chắn', 'nhà tôi', 'gần bên', 'bởi sao', 'tạo điều kiện', 'hoặc là', 'tự lượng', 'đến gần', 'sao vậy', 'đúng', 'khách', 'thật quả', 'ráo', 'thành thử', 'thà là', 'tại đây', 'nhưng', 'nước lên', 'ô kìa', 'để phần', 'mang nặng', 'lấy số', 'đầu tiên', 'rày', 'ớ này', 'thường thôi', 'lúc sáng', 'họ gần', 'biết việc', 'ào ào', 'tăng', 'bỏ mất', 'vài người', 'dành dành', 'vì vậy', 'như sau', 'chịu lời', 'khác', 'nghen', 'thường tại', 'biết trước', 'trong ấy', 'cũng nên', 'cả ăn', 'phỏng tính', 'quá', 'lại làm', 'đầy phè', 'ba ngày', 'nói xa', 'so với', 'thật sự', 'đồng thời', 'làm lấy', 'tay', 'mà', 'ngươi', 'mức', 'nào đâu', 'riu ríu', 'như chơi', 'chắc hẳn', 'rồi sau', 'quá thì', 'qua lại', 'dùng làm', 'chưa có', 'thậm chí', 'chí chết', 'khỏi nói', 'rồi xem', 'thực hiện đúng', 'sẽ biết', 'đâu đây', 'phè', 'chứ lại', 'từ đó', 'vượt khỏi', 'ra gì', 'ái dà', 'ba cùng', 'giống người', 'từng cái', 'bỏ mình', 'chẳng phải', 'tại đâu', 'vẫn', 'thì là', 'tiếp theo', 'thật lực', 'nhiên hậu', 'cấp trực tiếp', 'khiến', 'khỏi', 'bèn', 'phè phè', 'thật vậy', 'hay sao', 'ngồi', 'cứ như', 'dễ sợ', 'toẹt', 'mọi giờ', 'nói riêng', 'nhận biết', 'hay nói', 'quay bước', 'vô vàn', 'tính người', 'có nhà', 'mất', 'bất chợt', 'tháng ngày', 'trừ phi', 'nhỡ ra', 'ví dù', 'ầu ơ', 'có cơ', 'rằng', 'tự', 'tự tạo', 'chưa từng', 'ứ ừ', 'từ giờ', 'để lòng', 'ô kê', 'ngay thật', 'chị ấy', 'bỗng', 'trở thành', 'cây', 'bằng ấy', 'gây', 'đã lâu', 'trong ngoài', 'tắp lự', 'nhà việc', 'thế à', 'chợt', 'ồ ồ', 'chung cho', 'tự ăn', 'tạo ý', 'cảm ơn', 'chết tiệt', 'bằng không', 'của ngọt', 'nếu vậy', 'thêm vào', 'duy có', 'buổi', 'à ơi', 'sau chót', 'bằng vào', 'bây bẩy', 'nó', 'nhất là', 'bao nả', 'bất kể', 'thiếu gì', 'vì', 'á', 'nếu có', 'thực hiện', 'xa nhà', 'ăn cuộc', 'lâu các', 'nức nở', 'ơi', 'nhìn thấy', 'ăn trên', 'tuần tự', 'ngôi thứ', 'tốt bạn', 'thuộc từ', 'vở', 'như thường', 'thế ra', 'từ từ', 'phần', 'một cơn', 'ớ', 'ư', 'ngay lúc này', 'ngọn nguồn', 'gây ra', 'ái chà', 'có họ', 'thế lại', 'nghe nhìn', 'vùng nước', 'trên dưới', 'lấy để', 'gần ngày', 'cuối', 'vào đến', 'không cùng', 'như trên', 'tập trung', 'cho', 'hỏi', 'lý do', 'dễ gì', 'được cái', 'tôi con', 'xa cách', 'bởi vậy', 'ngõ hầu', 'giảm', 'xuể', 'ít thấy', 'biết chắc', 'hoàn toàn', 'ắt thật', 'cũng được', 'lúc ấy', 'số là', 'tạo ra', 'ối giời', 'những lúc', 'bấy', 'khi không', 'chớ kể', 'tấm các', 'tự ý', 'đến hay', 'số thiếu', 'nhỏ người', 'cần số', 'ăn hết', 'chung', 'nghe rõ', 'thường bị', 'không gì', 'nói ra', 'a lô', 'bài bác', 'vừa qua', 'thốt nhiên', 'cuốn', 'tới mức', 'vì rằng', 'lần sau', 'nghe', 'quả vậy', 'ít thôi', 'bất nhược', 'đầy năm', 'duy chỉ', 'cao ráo', 'xin vâng', 'tự tính', 'đã vậy', 'vào khoảng', 'của tin', 'ra', 'chỉ', 'phải chăng', 'trả trước', 'bằng nào', 'này nọ', 'bỏ ra', 'trước ngày', 'dùng', 'nữa khi', 'phải giờ', 'tăng thêm', 'nhất loạt', 'được', 'ơ hay', 'đến tuổi', 'bỏ quá', 'cho về', 'từng ấy', 'phải lời', 'úi dào', 'bước đi', 'không cứ', 'nhưng mà', 'thậm cấp', 'để cho', 'giờ đi', 'ra bài', 'thế nào', 'thực vậy', 'bỏ xa', 'quá bán', 'đến cùng', 'khó', 'rích', 'tất tật', 'lấy giống', 'thích tự', 'chớ như', 'cùng ăn', 'phần sau', 'ử', 'tức tốc', 'thấy tháng', 'bội phần', 'khi nào', 'đã thế', 'ngọt', 'ráo cả', 'ô hô', 'họ xa', 'tha hồ ăn', 'phỉ phui', 'tránh ra', 'ngày nọ', 'khó nói', 'ít biết', 'là phải', 'đâu đó', 'giờ này', 'một số', 'nền', 'người mình', 'rồi sao', 'lúc đi', 'đặt', 'bay biến', 'tự vì', 'cả thảy', 'rồi', 'nhanh tay', 'như thế', 'quá tin', 'lượng cả', 'cao xa', 'ăn tay', 'ngôi', 'đến', 'nếu như', 'tin thêm', 'đảm bảo', 'đây đó', 'choa', 'thế thế', 'ông nhỏ', 'ngộ nhỡ', 'bằng nhau', 'luôn cả', 'đối với', 'nhìn nhận', 'có đâu', 'ăn ngồi', 'dẫu', 'ái', 'như ai', 'tìm ra', 'rồi ra', 'năm tháng', 'một', 'nghe nói', 'hỗ trợ', 'hết ráo', 'làm ngay', 'veo', 'với nhau', 'làm tin', 'những là', 'bác', 'thoạt nghe', 'hết chuyện', 'nhược bằng', 'dào', 'vùng', 'lúc lâu', 'tay quay', 'thì phải', 'tăng chúng', 'dù dì', 'bằng nấy', 'ờ ờ', 'như', 'trả ngay', 'tuổi cả', 'đánh đùng', 'chịu tốt', 'phần nào', 'đưa tin', 'nhờ đó', 'thanh điểm', 'tôi', 'tại lòng', 'căn', 'lần lần', 'cho hay', 'không chỉ', 'vì chưng', 'mối', 'lần sang', 'chành chạnh', 'bỏ riêng', 'loại', 'thà', 'chưa dễ', 'đặt làm', 'bấy nay', 'phải tay', 'thấp cơ', 'cho tới khi', 'nghe tin', 'ơ', 'số người', 'đến điều', 'gần đến', 'khó nghĩ', 'hết cả', 'chú', 'đây này', 'trên bộ', 'song le', 'phía bên', 'lâu lâu', 'bằng', 'phứt', 'kể', 'tránh khỏi', 'ba', 'ít nhiều', 'hơn là', 'răng răng', 'bởi thế cho nên', 'nào là', 'tít mù', 'quan trọng', 'thực sự', 'với', 'mà cả', 'nay', 'đưa vào', 'lời chú', 'ăn quá', 'vô hình trung', 'có chăng là', 'bước tới', 'có được', 'sất', 'từ ái', 'tênh tênh', 'bỗng thấy', 'tạo nên', 'dưới', 'đến lúc', 'tỏ vẻ', 'bởi chưng', 'cái ấy', 'cô', 'thuộc cách', 'ào', 'lên cao', 'đặt để', 'nói đến', 'đặc biệt', 'nào cũng', 'ăn riêng', 'ấy', 'giá trị thực tế', 'hết nói', 'âu là', 'ở được', 'lại bộ', 'biết bao', 'lại', 'buổi làm', 'số loại', 'rén bước', 'nhớ', 'để không', 'đến cả', 'ba ngôi', 'hết', 'sau nữa', 'béng', 'phía trên', 'lấy cả', 'nhìn chung', 'chưa chắc', 'gây cho', 'nói lên', 'dưới nước', 'cho tin', 'mở', 'cả năm', 'phốc', 'cơ dẫn', 'hết rồi', 'chú khách', 'trả', 'về không', 'nghe ra', 'ngay lúc', 'đáng số', 'phải biết', 'đủ điều', 'nào hay', 'tại nơi', 'giống nhau', 'hoặc', 'tăm tắp', 'bỏ cuộc', 'ngay khi đến', 'bệt', 'bấy lâu', 'đều bước', 'ít khi', 'các cậu', 'cao', 'tà tà', 'không phải', 'mà vẫn', 'do đó', 'nhung nhăng', 'rằng là', 'làm', 'nhất định', 'vừa khi', 'đầy', 'ở', 'có điều', 'chắc vào', 'nhằm', 'từng nhà', 'ở vào', 'được lời', 'điều gì', 'biết được', 'chúng tôi', 'à', 'chỉ là', 'cả nghe', 'vừa', 'lần nào', 'nhận', 'phù hợp', 'bởi đâu', 'cho chắc', 'cho biết', 'cuối điểm', 'có dễ', 'khá tốt', 'có chuyện', 'thế sự', 'làm tại', 'quá lời', 'mọi người', 'ngày càng', 'xa gần', 'đủ số', 'thay đổi', 'tới thì', 'coi bộ', 'chọn', 'quá đáng', 'bất giác', 'đến đâu', 'thường khi', 'lúc', 'nguồn', 'lấy sau', 'đến nay', 'tin', 'cho rằng', 'thiếu', 'bộ', 'dạ dài', 'thanh', 'như tuồng', 'cho nên', 'cả thể', 'làm đúng', 'quả thật', 'cùng nhau', 'cứ điểm', 'giờ', 'rồi đây', 'một vài', 'đáng lý', 'vào vùng', 'bởi tại', 'chỉ tên', 'lượng', 'tuy nhiên', 'từng giờ', 'ngay', 'hay hay', 'có tháng', 'chính bản', 'đúng tuổi', 'dữ', 'làm tăng', 'chứ sao', 'cật sức', 'trước tiên', 'ý', 'bản riêng', 'ủa', 'còn', 'nhà ngoài', 'nhờ', 'ngồi trệt', 'gần hết', 'sao bằng', 'lúc khác', 'cũng vậy thôi', 'thường tính', 'tỏ ra', 'xăm xúi', 'quan tâm', 'tuy đã', 'dài lời', 'amen', 'tha hồ', 'nên tránh', 'cô ấy', 'phải người', 'nếu', 'thành ra', 'thích thuộc', 'về', 'chỉ chính', 'cha chả', 'để được', 'về sau', 'bỗng chốc', 'không dùng', 'ngồi sau', 'làm thế nào', 'thường đến', 'làm theo', 'trỏng', 'bản bộ', 'úi', 'dù cho', 'ngày xưa', 'khó nghe', 'tốc tả', 'vả lại', 'nấy', 'bởi thế', 'qua khỏi', 'rõ', 'tăng thế', 'ắt phải', 'oai oái', 'lần', 'bản ý', 'bao nhiêu', 'cũng thế', 'dầu sao', 'thời điểm', 'bây nhiêu', 'tên cái', 'nói trước', 'lúc đến', 'vốn dĩ', 'quá nhiều', 'chính là', 'sau này', 'dạ bán', 'phần lớn', 'đưa đến', 'bất thình lình', 'khác nào', 'áng như', 'như thế nào', 'chung cục', 'chưa kể', 'thỏm', 'tại vì', 'tấm bản', 'cùng tuổi', 'khó làm', 'khó biết', 'sau hết', 'kể từ', 'chúng ông', 'lên xuống', 'thật tốt', 'chính điểm', 'cùng tột', 'nhanh', 'hãy còn', 'từ khi', 'thếch', 'vừa vừa', 'thím', 'vượt', 'nói với', 'ai ai', 'từ điều', 'bấy chầy'}\n",
      "Processed with re pattern and stop words:\n",
      "                             input       output\n",
      "0        [Hello, ,, how, are, you]  [greetings]\n",
      "1  [I, am, fine, ,, thank, you, !]  [responses]\n",
      "\n",
      "Processed without re pattern but with stop words:\n",
      "                             input       output\n",
      "0     [Hello, ,, how, are, you, ?]  [greetings]\n",
      "1  [I, am, fine, ,, thank, you, !]  [responses]\n"
     ]
    }
   ],
   "source": [
    "with open(\"vietnamese.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    stop_words = set(line.strip() for line in file)\n",
    "print(stop_words)\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def preprocess_text(self, text, re_pattern=None, stop_words=None):\n",
    "        # words = word_tokenize(text)\n",
    "        if re_pattern:\n",
    "            pattern = \"[\" + re.escape(re_pattern) + \"]\"\n",
    "            text = re.sub(pattern, '', text)\n",
    "        words = word_tokenize(text)\n",
    "        if stop_words:\n",
    "            words = [word for word in words if word.lower() not in stop_words]\n",
    "        return words\n",
    "\n",
    "    def tokenize_df(self, df, input_column, output_column, re_pattern=None, stop_words=None):\n",
    "\n",
    "        df['input'] = df[input_column].apply(lambda x: self.preprocess_text(x, re_pattern, stop_words))\n",
    "        df['output'] = df[output_column].apply(lambda x: self.preprocess_text(x, re_pattern, stop_words))\n",
    "\n",
    "        df.drop(columns=[input_column, output_column], inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# With re pattern and stop words\n",
    "df_processed_with_re = tokenizer.tokenize_df(df.copy(), 'patterns0', 'tag', \"[.;\\]=]?\", stop_words)\n",
    "print(\"Processed with re pattern and stop words:\")\n",
    "print(df_processed_with_re[['input', 'output']])\n",
    "\n",
    "# Without re pattern but with stop words\n",
    "df_processed_with_stop_words = tokenizer.tokenize_df(df.copy(), 'patterns0', 'tag',  stop_words=stop_words)\n",
    "print(\"\\nProcessed without re pattern but with stop words:\")\n",
    "print(df_processed_with_stop_words[['input', 'output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class EmbeddiingInput(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocal_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x) *math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model :int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model =d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        \"\"\"creat a matrix of shape(seq_len, d_model)\"\"\"\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0,seq_len,dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float(*(-math.log(10000.0)/d_model)))\n",
    "        \n",
    "        pe[ :,0::2] = torch.sin(position * div_term)\n",
    "        pe[ :,1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x +(self.pe[ :,:x.shape(1):]).requires_gard_(False)\n",
    "        return self.Dropout(x)\n",
    "    \n",
    "class LayerNormalize(nn.Module):\n",
    "    def __init__(self, eps :float=10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim =-1,keepdim=True)\n",
    "        std = x.std(dim=-1,keepdim = True)\n",
    "        return self.alpha * (x -mean)/(std + self.eps) + self.bias\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)        \n",
    "            \n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalize()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))    \n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalize()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalize()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos:  PositionEncoding, tgt_pos:  PositionEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "\n",
    "class T5(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddiingInput(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEncoding(d_model, max_seq_len, dropout)\n",
    "        self.encoder = Encoder(self.make_encoder_layers(d_model, num_layers, num_heads, d_ff, dropout))\n",
    "        self.decoder = Decoder(self.make_decoder_layers(d_model, num_layers, num_heads, d_ff, dropout))\n",
    "        self.projection_layer = ProjectionLayer(d_model, vocab_size)\n",
    "\n",
    "    def make_encoder_layers(self, d_model, num_layers, num_heads, d_ff, dropout):\n",
    "        encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "            encoder_layers.append(encoder_block)\n",
    "        return nn.ModuleList(encoder_layers)\n",
    "\n",
    "    def make_decoder_layers(self, d_model, num_layers, num_heads, d_ff, dropout):\n",
    "        decoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            decoder_self_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "            decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, num_heads, dropout)\n",
    "            feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "            decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "            decoder_layers.append(decoder_block)\n",
    "        return nn.ModuleList(decoder_layers)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_embed = self.embedding(src)\n",
    "        src_embed = self.positional_encoding(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        tgt_embed = self.embedding(tgt)\n",
    "        tgt_embed = self.positional_encoding(tgt_embed)\n",
    "        decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        logits = self.projection_layer(decoder_output)\n",
    "        return logits\n",
    "\n",
    "# Sử dụng mô hình T5\n",
    "src_vocab_size = 10000  # Thay đổi tùy theo từ điển nguồn của bạn\n",
    "tgt_vocab_size = 10000  # Thay đổi tùy theo từ điển đích của bạn\n",
    "src_seq_len = 256       # Thay đổi tùy theo độ dài nguồn của bạn\n",
    "tgt_seq_len = 256       # Thay đổi tùy theo độ dài đích của bạn\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "max_seq_len = max(src_seq_len, tgt_seq_len)\n",
    "dropout = 0.1\n",
    "\n",
    "t5_model = T5(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_seq_len, dropout)\n",
    "\n",
    "# Sử dụng t5_model cho các tác vụ xử lý ngôn ngữ tự nhiên khác nhau\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMultiTask(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, src_seq_len, tgt_seq_len, num_classes, dropout=0.1):\n",
    "        super(TransformerMultiTask, self).__init__()\n",
    "\n",
    "        # Create separate embeddings for source and target\n",
    "        self.src_embedding = InputEmbeddings(d_model, src_vocab_size)\n",
    "        self.tgt_embedding = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "        # Create separate positional encoding layers for source and target\n",
    "        self.src_positional_encoding = PositionEncoding(d_model, src_seq_len, dropout=dropout)\n",
    "        self.tgt_positional_encoding = PositionEncoding(d_model, tgt_seq_len, dropout=dropout)\n",
    "\n",
    "        self.encoder = Encoder([EncoderBlock(\n",
    "            MultiHeadAttentionBlock(d_model, nhead, dropout=dropout),\n",
    "            FeedForwardBlock(d_model, dim_feedforward, dropout=dropout),\n",
    "            dropout=dropout\n",
    "        ) for _ in range(num_encoder_layers)])\n",
    "\n",
    "        self.decoder = Decoder([DecoderBlock(\n",
    "            MultiHeadAttentionBlock(d_model, nhead, dropout=dropout),\n",
    "            MultiHeadAttentionBlock(d_model, nhead, dropout=dropout),\n",
    "            FeedForwardBlock(d_model, dim_feedforward, dropout=dropout),\n",
    "            dropout=dropout\n",
    "        ) for _ in range(num_decoder_layers)])\n",
    "\n",
    "        # Classification layer for text classification task\n",
    "        self.classification_projection = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # Translation projection layer for language translation task\n",
    "        self.translation_projection = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "        # QA projection layer for question answering task\n",
    "        self.qa_projection = nn.Linear(d_model, 2)  # Predict start and end positions\n",
    "\n",
    "        # Text generation projection layer\n",
    "        self.text_generation_projection = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src_input_ids, tgt_input_ids, src_mask, tgt_mask, task):\n",
    "        src_embed = self.src_embedding(src_input_ids)\n",
    "        src_embed = src_embed + self.src_positional_encoding(src_embed)\n",
    "\n",
    "        tgt_embed = self.tgt_embedding(tgt_input_ids)\n",
    "        tgt_embed = tgt_embed + self.tgt_positional_encoding(tgt_embed)\n",
    "\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        if task == 'classification':\n",
    "            logits = self.classification_projection(encoder_output.mean(dim=1))\n",
    "            return logits\n",
    "        elif task == 'translation':\n",
    "            decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "            logits = self.translation_projection(decoder_output)\n",
    "            return logits\n",
    "        elif task == 'qa':\n",
    "            decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "            logits = self.qa_projection(decoder_output)\n",
    "            return logits\n",
    "        elif task == 'text_generation':\n",
    "            decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "            logits = self.text_generation_projection(decoder_output)\n",
    "            return logits\n",
    "        else:\n",
    "            raise ValueError(\"Task not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Xác định các siêu tham số huấn luyện\n",
    "src_vocab_size = 10000  # Kích thước từ vựng nguồn\n",
    "tgt_vocab_size = 10000  # Kích thước từ vựng đích\n",
    "d_model = 512  # Kích thước của vector nhúng\n",
    "nhead = 8  # Số lượng head trong multi-head attention\n",
    "num_encoder_layers = 6  # Số lớp mã hóa\n",
    "num_decoder_layers = 6  # Số lớp giải mã\n",
    "dim_feedforward = 2048  # Kích thước lớp feedforward ẩn\n",
    "src_seq_len = 50  # Độ dài tối đa của chuỗi đầu vào\n",
    "tgt_seq_len = 60  # Độ dài tối đa của chuỗi đầu ra\n",
    "num_classes = 10  # Số lớp trong tác vụ phân loại\n",
    "dropout = 0.1  # Tỷ lệ dropout\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = TransformerMultiTask(src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, src_seq_len, tgt_seq_len, num_classes, dropout=dropout)\n",
    "\n",
    "# Xác định hàm mất mát và tối ưu hóa\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dữ liệu mẫu (bạn cần thay thế bằng dữ liệu thực tế)\n",
    "train_src_input_ids = torch.randint(0, src_vocab_size, (100, src_seq_len))  # Mã hóa nguồn\n",
    "train_tgt_input_ids = torch.randint(0, tgt_vocab_size, (100, tgt_seq_len))  # Mã hóa đích\n",
    "train_src_mask = torch.ones(100, src_seq_len).bool()  # Mặt nạ nguồn\n",
    "train_tgt_mask = torch.ones(100, tgt_seq_len).bool()  # Mặt nạ đích\n",
    "train_labels = torch.randint(0, num_classes, (100,))  # Nhãn phân loại\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Chuyển sang chế độ huấn luyện\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADecoderHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.start_token_projection = nn.Linear(d_model, 1)  # Dự đoán vị trí bắt đầu\n",
    "        self.end_token_projection = nn.Linear(d_model, 1)    # Dự đoán vị trí kết thúc\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Đầu ra cho câu hỏi và trả lời\n",
    "        start_logits = self.start_token_projection(x).squeeze(-1)  # Loại bỏ kích thước cuối cùng\n",
    "        end_logits = self.end_token_projection(x).squeeze(-1)      # Loại bỏ kích thước cuối cùng\n",
    "        \n",
    "        # Áp dụng softmax để tính xác suất vị trí bắt đầu và kết thúc\n",
    "        start_probs = self.softmax(start_logits)\n",
    "        end_probs = self.softmax(end_logits)\n",
    "        \n",
    "        return start_probs, end_probs\n",
    "class TextGenerationHead(nn.Module):\n",
    "    def __init__(self, d_model, max_output_length, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_output_length = max_output_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.decoder_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Đầu ra cho tạo văn bản\n",
    "        logits = self.decoder_layer(x)\n",
    "        return logits\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos:  PositionEncoding, tgt_pos:  PositionEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "class TransformerTasks(nn.Module):\n",
    "    \n",
    "    def __init__(self, task, src_vocab_size: int, tgt_vocab_size: int,src_seq_len: int, tgt_seq_len: int,d_model: int=512, N: int=6, h: int=8, dropout: float=0.1,d_ff: int=2048, num_classes= None)-> Transformer:\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input Embedding\n",
    "        self.src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "        self.tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.src_pos = PositionEncoding(d_model, src_seq_len, dropout)\n",
    "        self.tgt_pos = PositionEncoding(d_model, tgt_seq_len, dropout)\n",
    "        \n",
    "        # Encoder and Decoder layers (you can customize these as needed)\n",
    "        self.encoder = Encoder([EncoderBlock(MultiHeadAttentionBlock(d_model, h, dropout),\n",
    "                                             FeedForwardBlock(d_model, d_ff, dropout),dropout) for _ in range(N)])\n",
    "        \n",
    "        self.decoder = Decoder([DecoderBlock(MultiHeadAttentionBlock(d_model, h, dropout),\n",
    "                                             MultiHeadAttentionBlock(d_model, h, dropout),\n",
    "                                             FeedForwardBlock(d_model, d_ff, dropout),\n",
    "                                             dropout) for _ in range(N)])\n",
    "         # Create the encoder and decoder\n",
    "        self.encoder = Encoder(nn.ModuleList(self.encoder))\n",
    "        self.decoder = Decoder(nn.ModuleList(self.decoder))\n",
    "        \n",
    "\n",
    "        projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "        if task == \"classification\":\n",
    "            # Nhiệm vụ phân loại văn bản\n",
    "            self.classification_layer = nn.Linear(d_model, num_classes)\n",
    "        elif task == \"translation\":\n",
    "            # Nhiệm vụ dịch tiếng\n",
    "            self.projection_layer = ProjectionLayer(d_model, src_vocab_size)\n",
    "        elif task == \"qa\":\n",
    "            # Nhiệm vụ câu hỏi và trả lời\n",
    "            self.qa_decoder = QADecoderHead(d_model)\n",
    "        elif task == \"summarization\":\n",
    "            # Nhiệm vụ tóm tắt văn bản\n",
    "            self.projection_layer = ProjectionLayer(d_model, src_vocab_size)\n",
    "        elif task == \"generation\":\n",
    "            # Nhiệm vụ tạo văn bản\n",
    "            self.text_generation_head = TextGenerationHead(d_model, max_output_length,src_vocab_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task specified\")\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src_embedded = self.src_embed(src) * math.sqrt(self.d_model)\n",
    "        src_encoded = self.encoder(self.src_pos(src_embedded), src_mask)\n",
    "        \n",
    "        tgt_embedded = self.tgt_embed(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_pos_encoded = self.tgt_pos(tgt_embedded)\n",
    "        \n",
    "        if self.task == \"classification\":\n",
    "            # Nhiệm vụ phân loại văn bản\n",
    "            cls_representation = src_encoded[:, 0, :]\n",
    "            logits = self.classification_layer(cls_representation)\n",
    "            return logits\n",
    "        elif self.task == \"translation\":\n",
    "            # Nhiệm vụ dịch tiếng\n",
    "            translation_output = self.decoder(tgt_pos_encoded, src_encoded, src_mask, tgt_mask)\n",
    "            translation_logits = self.projection_layer(translation_output)\n",
    "            return translation_logits\n",
    "        elif self.task == \"qa\":\n",
    "            # Nhiệm vụ câu hỏi và trả lời\n",
    "            qa_output = self.qa_decoder(tgt_pos_encoded)\n",
    "            return qa_output\n",
    "        elif self.task == \"summarization\":\n",
    "            # Nhiệm vụ tóm tắt văn bản\n",
    "            summarization_output = self.decoder(tgt_pos_encoded, src_encoded, src_mask, tgt_mask)\n",
    "            summarization_logits = self.projection_layer(summarization_output)\n",
    "            return summarization_logits\n",
    "        elif self.task == \"generation\":\n",
    "            # Nhiệm vụ tạo văn bản\n",
    "            generation_output = self.decoder(tgt_pos_encoded, src_encoded, src_mask, tgt_mask)\n",
    "            generation_logits = self.text_generation_head(generation_output)\n",
    "            return generation_logits\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5]])\n",
      "tensor([[[1, 2, 3, 4, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tạo một vector có kích thước (5,)\n",
    "vector = torch.tensor([[1, 2, 3, 4, 5],[1, 2, 3, 4, 5]])\n",
    "print(vector) \n",
    "# Sử dụng unsqueeze(1) để chuyển thành ma trận cột\n",
    "matrix_column = vector.unsqueeze(1)\n",
    "\n",
    "# In kích thước của ma trận cột\n",
    "print(matrix_column)  # Output: torch.Size([5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tạo một danh sách các câu văn bản từ dữ liệu\n",
    "sentences = [['this', 'is', 'an', 'example', 'sentence'],\n",
    "            ['another', 'example', 'sentence'],\n",
    "            ['yet', 'another', 'sentence']]\n",
    "\n",
    "# Huấn luyện mô hình Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Lưu mô hình để sử dụng sau này\n",
    "model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
     ]
    }
   ],
   "source": [
    "# Tìm vectơ biểu diễn của từ \"example\"\n",
    "vector = model.wv[\"example\"]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5ForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Khởi tạo mô hình và tokenizer T5\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Định nghĩa dữ liệu huấn luyện dạng Dataset cho cả hai tác vụ\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, is_classification=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_classification = is_classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        if self.is_classification:\n",
    "            return {\n",
    "                \"input_ids\": encoding.input_ids.squeeze(),\n",
    "                \"attention_mask\": encoding.attention_mask.squeeze(),\n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": encoding.input_ids.squeeze(),\n",
    "                \"attention_mask\": encoding.attention_mask.squeeze(),\n",
    "                \"decoder_input_ids\": encoding.input_ids.squeeze(),\n",
    "                \"decoder_attention_mask\": encoding.attention_mask.squeeze(),\n",
    "            }\n",
    "\n",
    "# Dữ liệu cho tác vụ \"Tạo văn bản\"\n",
    "texts_generation = [\"This is a positive example.\", \"This is a negative example.\", \"This is another positive example.\"]\n",
    "labels_generation = [\"This is a positive generated text.\", \"This is a negative generated text.\", \"This is another positive generated text.\"]\n",
    "\n",
    "# Dữ liệu cho tác vụ \"Phân loại văn bản\"\n",
    "texts_classification = [\"This is a positive text.\", \"This is a negative text.\", \"This is another positive text.\"]\n",
    "labels_classification = [1, 0, 1]  # 1 cho tích cực, 0 cho tiêu cực\n",
    "\n",
    "# Tạo datasets\n",
    "generation_dataset = MultiTaskDataset(texts_generation, labels_generation, tokenizer, is_classification=False)\n",
    "classification_dataset = MultiTaskDataset(texts_classification, labels_classification, tokenizer, is_classification=True)\n",
    "\n",
    "# DataLoader để tải dữ liệu và xử lý batch cho cả hai tác vụ\n",
    "generation_dataloader = DataLoader(generation_dataset, batch_size=2, shuffle=True)\n",
    "classification_dataloader = DataLoader(classification_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Khởi tạo mô hình T5 cho tác vụ \"Tạo văn bản\"\n",
    "generation_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Khởi tạo mô hình T5 cho tác vụ \"Phân loại văn bản\"\n",
    "classification_model = T5ForSequenceClassification.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Huấn luyện cả hai tác vụ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generation_model.to(device)\n",
    "classification_model.to(device)\n",
    "\n",
    "optimizer_generation = AdamW(generation_model.parameters(), lr=1e-4)\n",
    "optimizer_classification = AdamW(classification_model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_generation(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=decoder_input_ids\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_classification(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Huấn luyện mô hình cho tác vụ \"Tạo văn bản\"\n",
    "for epoch in range(5):\n",
    "    loss = train_generation(generation_model, generation_dataloader, optimizer_generation, device)\n",
    "    print(f\"Epoch {epoch + 1} - Text Generation Loss: {loss:.4f}\")\n",
    "\n",
    "# Huấn luyện mô hình cho tác vụ \"Phân loại văn bản\"\n",
    "for epoch in range(5):\n",
    "    loss = train_classification(classification_model, classification_dataloader, optimizer_classification, device)\n",
    "    print(f\"Epoch {epoch + 1} - Text Classification Loss: {loss:.4f}\")\n",
    "\n",
    "# Lưu mô hình đã huấn luyện cho cả hai tác vụ\n",
    "generation_model.save_pretrained(\"text_generation_model\")\n",
    "classification_model.save_pretrained(\"text_classification_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
